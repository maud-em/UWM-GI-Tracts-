{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [UW-Madison GI Tract Image Segmentation](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/)\n> Track healthy organs in medical scans to improve cancer treatment\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25\">","metadata":{}},{"cell_type":"markdown","source":"# Methodlogy \n* In this notebook **2.5D** images are used for Training for **Segmentation** with `tf.data`, `tfrecord` using `Tensorflow`.  \n* In a nutshell, **2.5D Image Training** is training of **3D** image like **2D** Image. 2.5D images can take leverage of the extra depth information like our typical RGB image. 2.5D Images are built from 3 channels with 2 strides \n* The TransUNet model from **[TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation](https://arxiv.org/pdf/2102.04306.pdf)** is used here (from the transunet library).\n* The model has 100M parameters that we need to train. To use TPU capabilities, the dataset has to be transformed into a TFRecord. I used the 2.5D image dataset created in this notebook by awsaf49: [UWMGI: 2.5D TFRecord Data](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-tfrecord-data).\n* \"TFRecord files are created using **StratifiedGroupFold** to avoid data leakage due to `case` and to stratify `empty` and `non-empty` mask cases\".\n* This notebook is compatible for both **GPU** and **TPU**. Device is automatically selected so you won't have to do anything to allocate device.\n* As there are overlaps between **Stomach**, **Large Bowel** & **Small Bowel** classes, this is a **MultiLabel Segmentation** task, so final activaion should be `sigmoid` instead of `softmax`.\n* You can play with different models and losses.","metadata":{}},{"cell_type":"markdown","source":"# Reference Notebooks and Datasets \nðŸ“Œ **2.5D-TransUNet**:\n* Train: [UWMGI: TransUnet 2.5D [Train] [TF]](https://www.kaggle.com/awsaf49/uwmgi-transunet-2-5d-train-tf/)\n<!-- * Infer:  UWMGI: TransUnet 2.5D [Infer] [TF]-->\n\nðŸ“Œ **Data/Dataset**:\n* Dataset: [UWMGI: 2.5D TFRecord Dataset](https://www.kaggle.com/datasets/awsaf49/uwmgi-25d-tfrecord-dataset)","metadata":{}},{"cell_type":"markdown","source":"# Install Libraries \nUsing transunet and segmentation_models library","metadata":{}},{"cell_type":"code","source":"!pip install  transunet\n!pip install  segmentation_models\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-28T17:18:57.581206Z","iopub.execute_input":"2022-06-28T17:18:57.581729Z","iopub.status.idle":"2022-06-28T17:19:13.163112Z","shell.execute_reply.started":"2022-06-28T17:18:57.581685Z","shell.execute_reply":"2022-06-28T17:19:13.162125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries \n","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np, random,os, shutil\nimport tensorflow as tf, re, math\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nimport sklearn\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport tensorflow_addons as tfa\nimport yaml\nfrom IPython import display as ipd\nimport json\nfrom datetime import datetime\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom IPython import display as ipd\n\nimport scipy\nimport warnings\n\ntfk = tf.keras\ntfkl = tfk.layers\ntfm = tf.math\n\n# Show less log messages\ntf.get_logger().setLevel('ERROR')\ntf.autograph.set_verbosity(0)\n\n# Set tf.keras as backend\nos.environ['SM_FRAMEWORK'] = 'tf.keras'\nimport segmentation_models as sm\n\n# Set true to show less logging messages\nos.environ[\"WANDB_SILENT\"] = \"false\"\nimport wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-28T17:19:40.861636Z","iopub.execute_input":"2022-06-28T17:19:40.86246Z","iopub.status.idle":"2022-06-28T17:19:40.874457Z","shell.execute_reply.started":"2022-06-28T17:19:40.86241Z","shell.execute_reply":"2022-06-28T17:19:40.873648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration \nThis is where the hyper parameters and the optimizer are set","metadata":{}},{"cell_type":"code","source":"class CFG:\n    competition = \"uwmgi-tf\"\n\n    debug = False\n    exp_name = \"v4\"\n    comment = \"TransUNet-ResNet50V2-128x128-aug-2.5D\"\n\n    # Use verbose=0 for silent, 1 for interactive\n    verbose = 0\n    display_plot = True\n\n    # Device for training\n    device = None  # device is automatically selected\n\n    # Model & Backbone\n    model_name = \"TransUNet\"\n    backbone = \"ResNet50V2\"\n\n    # Seeding for reproducibility\n    seed = 101\n\n    # Number of folds\n    folds = 5\n\n    # Which Folds to train\n    selected_folds = [0, 1, 2, 3, 4]\n\n    # Image Size\n    img_size = [128, 128]\n\n    # Batch Size & Epochs\n    batch_size = 32\n    drop_remainder = False\n    epochs = 15\n    steps_per_execution = None\n\n    # Loss & Optimizer & LR Scheduler\n    loss = \"dice_loss\"\n    optimizer = \"Adam\"\n    lr = 5e-4\n    lr_schedule = \"CosineDecay\"\n    patience = 5\n\n    # Augmentation\n    augment = True\n    transform = False\n\n    # Transformation\n    fill_mode = \"constant\"\n    rot = 5.0  # proprtional\n    shr = 5.0  # proprtional\n    hzoom = 100.0  # inv proportional\n    wzoom = 100.0  # inv proportional\n    hshift = 10.0  # proportional\n    wshift = 10.0  # proportional\n\n    # Horizontal & Vertical Flip\n    hflip = 0.5\n    vflip = 0.5\n\n    # Clip values to [0, 1]\n    clip = False\n\n    # CutOut\n    drop_prob = 0.5\n    drop_cnt = 10\n    drop_size = 0.05\n\n    # Jitter\n    sat = [0.7, 1.3]  # saturation\n    cont = [0.8, 1.2]  # contrast\n    bri = 0.15  # brightness\n    hue = 0.0  # hue\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:19:45.311893Z","iopub.execute_input":"2022-06-28T17:19:45.312146Z","iopub.status.idle":"2022-06-28T17:19:45.32299Z","shell.execute_reply.started":"2022-06-28T17:19:45.312122Z","shell.execute_reply":"2022-06-28T17:19:45.322082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set Up Device \nThe following will automatically detects hardware(tpu or gpu or cpu). ","metadata":{}},{"cell_type":"code","source":"try: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # for CPU/GPU or multi-GPU machines\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:21:29.741842Z","iopub.execute_input":"2022-06-28T17:21:29.742169Z","iopub.status.idle":"2022-06-28T17:21:36.204224Z","shell.execute_reply.started":"2022-06-28T17:21:29.742139Z","shell.execute_reply":"2022-06-28T17:21:36.203539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#strategy, CFG.device, tpu = configure_device()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:21:45.901105Z","iopub.execute_input":"2022-06-28T17:21:45.901373Z","iopub.status.idle":"2022-06-28T17:21:45.906839Z","shell.execute_reply.started":"2022-06-28T17:21:45.901347Z","shell.execute_reply":"2022-06-28T17:21:45.905967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meta Data\n* Files\n    * `train.csv` - IDs and masks for all training objects.\n    * `sample_submission.csv` - a sample submission file in the correct format\n    * `train/` - a folder of case/day folders, each containing slice images for a particular case on a given day.\n\n> **Note** that the image filenames include 4 numbers (ex. `276_276_1.63_1.63.png`). These four numbers are slice height / width (integers in pixels) and heigh/width pixel spacing (floating points in mm). The first two defines the resolution of the slide. The last two record the physical size of each pixel.\n\n* Columns\n    * `id` - unique identifier for object\n    * `class` - the predicted class for the object\n    * `EncodedPixels` - RLE-encoded pixels for the identified object","metadata":{}},{"cell_type":"code","source":"import re\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-28T17:21:54.118982Z","iopub.execute_input":"2022-06-28T17:21:54.119958Z","iopub.status.idle":"2022-06-28T17:21:54.125078Z","shell.execute_reply.started":"2022-06-28T17:21:54.119917Z","shell.execute_reply":"2022-06-28T17:21:54.124234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training on TPU: Kaggle over Colab**\nTo run code on **TPU** we need our private data to be stored on **Google Cloud Storage**. Hence, we'll be needing **GCS_PATH** of our stored data. \n* Kaggle provides a **GCS_PATH** for public datasets. Hence we can use it for training our model on **TPU**. Simply we have to use `KaggleDatasets()` to get `GCS_PATH` of our dataset.\n* It is much more complicated on Colab, and it is better to stick with Kaggle here","metadata":{}},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/uw-madison-gi-tract-image-segmentation'\nGCS_PATH = KaggleDatasets().get_gcs_path('uwmgi-25d-tfrecord-dataset')\nALL_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/uwmgi/*.tfrec')\nprint('NUM TFRECORD FILES: {:,}'.format(len(ALL_FILENAMES)))\nprint('NUM TRAINING IMAGES: {:,}'.format(count_data_items(ALL_FILENAMES)))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:21:57.155367Z","iopub.execute_input":"2022-06-28T17:21:57.15575Z","iopub.status.idle":"2022-06-28T17:21:57.74927Z","shell.execute_reply.started":"2022-06-28T17:21:57.155693Z","shell.execute_reply":"2022-06-28T17:21:57.748289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation \nUnlike classification problem, we have to augment both **image** & **mask** otherwise it'll create faulty data as **mask** won't match its corresponding **image**.\n","metadata":{}},{"cell_type":"markdown","source":"## Used Augmentations\nSome Augmentations that were used here are,\n\n* RandomFlip (Left-Right)\n<img src=\"https://dataaspirant.com/wp-content/uploads/2020/08/5-horizontal-flip-technique.png\" width=400>\n\n* Random Rotation\n<img src=\"https://dataaspirant.com/wp-content/uploads/2020/08/4-rotation-technique.png\" width=500>\n\n\n* RandomBrightness\n<img src=\"https://affine.ai/wp-content/uploads/2022/02/2.jpg\" width=400>\n\n* RndomContrast\n<img src=\"https://affine.ai/wp-content/uploads/2022/02/3.jpg\" width=400>\n\n* Zoom\n<img src=\"https://affine.ai/wp-content/uploads/2022/02/16.jpg\" width=400>\n\n* Cutout\n<img src=\"https://i.ibb.co/3MKjW0t/cutout.png\" width=400>\n\n* Shear\n<img src=\"https://imgaug.readthedocs.io/en/latest/_images/shearx.jpg\" width=500>","metadata":{}},{"cell_type":"markdown","source":"## Utility","metadata":{}},{"cell_type":"code","source":"def random_int(shape=[], minval=0, maxval=1):\n    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n\n\ndef random_float(shape=[], minval=0.0, maxval=1.0):\n    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n    return rnd\n\n\ndef get_mat(shear, height_zoom, width_zoom, height_shift, width_shift):\n    \"\"\"\n    ref: https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords/\n    \"\"\"\n    # returns 3x3 transformmatrix which transforms indicies\n\n    # CONVERT DEGREES TO RADIANS\n    # rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.0\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst], axis=0), [3, 3])\n\n    one = tf.constant([1], dtype=\"float32\")\n    zero = tf.constant([0], dtype=\"float32\")\n\n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n\n    shear_matrix = get_3x3_mat([one, s2, zero, zero, c2, zero, zero, zero, one])\n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat(\n        [one / height_zoom, zero, zero, zero, one / width_zoom, zero, zero, zero, one]\n    )\n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat(\n        [one, zero, height_shift, zero, one, width_shift, zero, zero, one]\n    )\n\n    return K.dot(\n        shear_matrix, K.dot(zoom_matrix, shift_matrix)\n    )  # K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-28T17:22:09.080212Z","iopub.execute_input":"2022-06-28T17:22:09.080493Z","iopub.status.idle":"2022-06-28T17:22:09.092222Z","shell.execute_reply.started":"2022-06-28T17:22:09.080458Z","shell.execute_reply":"2022-06-28T17:22:09.091351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augment Fn","metadata":{}},{"cell_type":"code","source":"def ShiftScaleRotate(image, mask=None, DIM=CFG.img_size, p=1.0):\n    \"\"\"\n    ref: https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords/\n    \"\"\"\n    if random_float() > p:\n        return image, mask\n    if DIM[0] > DIM[1]:\n        diff = DIM[0] - DIM[1]\n        pad = [diff // 2, diff // 2 + (1 if diff % 2 else 0)]\n        image = tf.pad(image, [[0, 0], [pad[0], pad[1]], [0, 0]])\n        NEW_DIM = DIM[0]\n        if mask is not None:\n            mask = tf.pad(mask, [[0, 0], [pad[0], pad[1]], [0, 0]])\n    elif DIM[0] < DIM[1]:\n        diff = DIM[1] - DIM[0]\n        pad = [diff // 2, diff // 2 + (1 if diff % 2 else 0)]\n        image = tf.pad(image, [[pad[0], pad[1]], [0, 0], [0, 0]])\n        NEW_DIM = DIM[1]\n        if mask is not None:\n            mask = tf.pad(mask, [[pad[0], pad[1]], [0, 0], [0, 0]])\n\n    rot = CFG.rot * tf.random.normal([1], dtype=\"float32\")\n    shr = CFG.shr * tf.random.normal([1], dtype=\"float32\")\n    h_zoom = 1.0 + tf.random.normal([1], dtype=\"float32\") / CFG.hzoom\n    w_zoom = 1.0 + tf.random.normal([1], dtype=\"float32\") / CFG.wzoom\n    h_shift = CFG.hshift * tf.random.normal([1], dtype=\"float32\")\n    w_shift = CFG.wshift * tf.random.normal([1], dtype=\"float32\")\n\n    transformation_matrix = tf.linalg.inv(\n        get_mat(shr, h_zoom, w_zoom, h_shift, w_shift)\n    )\n    flat_tensor = tfa.image.transform_ops.matrices_to_flat_transforms(\n        transformation_matrix\n    )\n    rotation = math.pi * rot / 180.0\n\n    image = tfa.image.transform(image, flat_tensor, fill_mode=CFG.fill_mode)\n    image = tfa.image.rotate(image, -rotation, fill_mode=CFG.fill_mode)\n    if mask is not None:\n        mask = tfa.image.transform(mask, flat_tensor, fill_mode=CFG.fill_mode)\n        mask = tfa.image.rotate(mask, -rotation, fill_mode=CFG.fill_mode)\n\n    if DIM[0] > DIM[1]:\n        image = tf.reshape(image, [NEW_DIM, NEW_DIM, 3])\n        image = image[:, pad[0] : -pad[1], :]\n        if mask is not None:\n            mask = tf.reshape(mask, [NEW_DIM, NEW_DIM, 3])\n            mask = mask[:, pad[0] : -pad[1], :]\n    elif DIM[1] > DIM[0]:\n        image = tf.reshape(image, [NEW_DIM, NEW_DIM, 3])\n        image = image[pad[0] : -pad[1], :, :]\n        if mask is not None:\n            mask = tf.reshape(mask, [NEW_DIM, NEW_DIM, 3])\n            mask = mask[pad[0] : -pad[1], :, :]\n\n    image = tf.reshape(image, [*DIM, 3])\n    if mask is not None:\n        mask = tf.reshape(mask, [*DIM, 3])\n    return image, mask\n\n\ndef CutOut(image, mask=None, DIM=CFG.img_size, PROBABILITY=0.6, CT=5, SZ=0.1):\n    \"\"\"\n    ref: https://www.kaggle.com/code/cdeotte/tfrecord-experiments-upsample-and-coarse-dropout\n    \"\"\"\n    # Input Image - is with shape [dim,dim,3] not of [None,dim,dim,3]\n    # Probability\n    P = tf.cast(random_float() < PROBABILITY, tf.int32)\n    if (P == 0) | (CT == 0) | (SZ == 0):\n        return image, mask\n    # Iterate Through Each Sample of Batch\n    for k in range(CT):\n        # Choose Random Location\n        x = tf.cast(tf.random.uniform([], 0, DIM[1]), tf.int32)\n        y = tf.cast(tf.random.uniform([], 0, DIM[0]), tf.int32)\n        # Compute Square for CutOut\n        WIDTH = tf.cast(SZ * min(DIM), tf.int32) * P\n        ya = tf.math.maximum(0, y - WIDTH // 2)\n        yb = tf.math.minimum(DIM[0], y + WIDTH // 2)\n        xa = tf.math.maximum(0, x - WIDTH // 2)\n        xb = tf.math.minimum(DIM[1], x + WIDTH // 2)\n        # CutOut Image\n        one = image[ya:yb, 0:xa, :]\n        two = tf.zeros([yb - ya, xb - xa, 3], dtype=image.dtype)\n        three = image[ya:yb, xb : DIM[1], :]\n        middle = tf.concat([one, two, three], axis=1)\n        image = tf.concat([image[0:ya, :, :], middle, image[yb : DIM[0], :, :]], axis=0)\n        image = tf.reshape(image, [*DIM, 3])\n        # CutOut Mask\n        if mask is not None:\n            one = mask[ya:yb, 0:xa, :]\n            two = tf.zeros([yb - ya, xb - xa, 3], dtype=mask.dtype)  # ch=3\n            three = mask[ya:yb, xb : DIM[1], :]\n            middle = tf.concat([one, two, three], axis=1)\n            mask = tf.concat(\n                [mask[0:ya, :, :], middle, mask[yb : DIM[0], :, :]], axis=0\n            )\n            mask = tf.reshape(mask, [*DIM, 3])  # ch=3\n    return image, mask\n\n\ndef RandomJitter(img, hue, sat, cont, bri, p=1.0):\n    if random_float() > p:\n        return img\n    img = tf.image.random_hue(img, hue)\n    img = tf.image.random_saturation(img, sat[0], sat[1])\n    img = tf.image.random_contrast(img, cont[0], cont[1])\n    img = tf.image.random_brightness(img, bri)\n    return img\n\n\ndef RandomFlip(img, msk=None, hflip_p=0.5, vflip_p=0.5):\n    if random_float() < hflip_p:\n        img = tf.image.flip_left_right(img)\n        if msk is not None:\n            msk = tf.image.flip_left_right(msk)\n    if random_float() < vflip_p:\n        img = tf.image.flip_up_down(img)\n        if msk is not None:\n            msk = tf.image.flip_up_down(msk)\n    return img, msk","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-28T17:23:05.117516Z","iopub.execute_input":"2022-06-28T17:23:05.117855Z","iopub.status.idle":"2022-06-28T17:23:05.157243Z","shell.execute_reply.started":"2022-06-28T17:23:05.117824Z","shell.execute_reply":"2022-06-28T17:23:05.15629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline ","metadata":{}},{"cell_type":"markdown","source":"## 2.5D Training\n**What is 2.5D Training?**\n\nEven though we can do easy straight-forward 2D training, we can utilize ct slices for extra depth information. For example, we can stack consecutive slices of the scans to get a 3D volume. But one of the reasons why I'm inferring them as 2.5D is that we'll be training 3D images like 2D images. Those who haven't come across this method may get confused at first but let me explain. When we train 2D images like RGB images we actually pass a 3D tensor ex:`[None, channel, height, width]` to a model. For PyTorch, the last two dimensions are spacial(height & width) and the first one is the **channel** dimension. Now for the ct image, we don't have any channel information so we can use that dimension to **stack multiple ct scans as channels and train them as 2d images**. \n\nThis method has some cool advantages over 3D training for instance,\n* Low GPU/memory cost.\n* Simple pipeline.\n* Easier augmentation.\n* Quick inference.\n* Many open-source models.\n\nIn my notebook, I've stacked 3 slices with stride=2, you can check the demo image above for example. It kinda looks like **3d movie scene in the theatre**. \n\n<div align=center><img src=\"https://i.ibb.co/sgsPf4v/Capture.png\" width=800></div>\n<div align=center><img src=\"https://i.ibb.co/KKtZ7Gn/Picture1-3d.png\" width=500></div>","metadata":{}},{"cell_type":"markdown","source":"## Reading TFRecord Data\n**What is TFRecord & Why use it for Segmentation?**\n\n* The `.tfrecord`/`.tfrec` format is TensorFlow's custom data format which is used for storing a sequence of binary records.\n* For **Segmentation** unlike any other data formatk in `.tfrecord` we don't have to the read file twice (one for image and one for mask). In `tfrecord` we just have to read file once and we can access both image and mask.\n* TFRecord consumes **less storage on disk**, and has **faster read and write time from the disk**, which makes it suitable for **segmentation** task.\n* Apart from that there are a number of advantages to using TFRecords: \n    * Efficient usage of storage.\n    * Better I/O Speed.\n    * TPUs require that you pass data to them in TFRecord format\n    \n**How TFRecord is created for Segmentation?**\n\n* Mask is stored in `tfrecord` exactly the same way as a image that is as a byte-string. \n* So, you can easily access the both image and mask from example_proto using `exmple[\"image\"]` & `example[\"mask\"]`. \n* Then, to decode it to `tf.Tensor` simply we can use `tf.io.decode_raw()` function.\n* For more information, checout [UWMGI: 2.5D TFRecord Data](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-tfrecord-data) notebook.","metadata":{}},{"cell_type":"code","source":"# Decode image from bytestring to tensor\ndef decode_image(data, height, width, target_size=CFG.img_size):\n    img = tf.io.decode_raw(data, out_type=tf.uint16)\n    img = tf.reshape(img, [height, width, 3])  # explicit size needed for TPU\n    img = tf.cast(img, tf.float32)\n    img = tf.math.divide_no_nan(img, tf.math.reduce_max(img))  # scale image to [0, 1]\n    img = tf.image.resize_with_pad(\n        img, target_size[0], target_size[1], method=\"nearest\"\n    )  # resize with pad to avoid distortion\n    img = tf.reshape(img, [*target_size, 3])  # reshape after resize\n    return img\n\n\n# Decode mask from bytestring to tensor\ndef decode_mask(data, height, width, target_size=CFG.img_size):\n    msk = tf.io.decode_raw(data, out_type=tf.uint8)\n    msk = tf.reshape(msk, [height, width, 3])  # explicit size needed for TPU\n    msk = tf.cast(msk, tf.float32)\n    msk = msk / 255.0  # scale mask data to[0, 1]\n    msk = tf.image.resize_with_pad(\n        msk, target_size[0], target_size[1], method=\"nearest\"\n    )\n    msk = tf.reshape(msk, [*target_size, 3])  # reshape after resize\n    return msk\n\n\n# Read tfrecord data & parse it & do augmentation\ndef read_tfrecord(example, augment=True, return_id=False, dim=CFG.img_size):\n    tfrec_format = {\n        \"id\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n        \"height\": tf.io.FixedLenFeature([], tf.int64),\n        \"width\": tf.io.FixedLenFeature([], tf.int64),\n        \"mask\": tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(\n        example, tfrec_format\n    )  # parses a single example proto.\n    image_id = example[\"id\"]\n    height = example[\"height\"]\n    width = example[\"width\"]\n    img = decode_image(example[\"image\"], height, width, dim)  # access image\n    msk = decode_mask(example[\"mask\"], height, width, dim)  # access mask\n    if augment:  # do augmentation\n        img, msk = ShiftScaleRotate(img, msk, DIM=dim, p=0.75)\n        img, msk = RandomFlip(img, msk, hflip_p=CFG.hflip, vflip_p=CFG.vflip)\n        img = RandomJitter(img, CFG.hue, CFG.sat, CFG.cont, CFG.bri, p=0.8)\n        img, msk = CutOut(\n            img,\n            msk,\n            DIM=dim,\n            PROBABILITY=CFG.drop_prob,\n            CT=CFG.drop_cnt,\n            SZ=CFG.drop_size,\n        )\n    img = tf.clip_by_value(img, 0, 1) if CFG.clip else img\n    img = tf.reshape(img, [*dim, 3])\n    msk = tf.reshape(msk, [*dim, 3])\n    return (img, msk) if not return_id else (img, image_id, msk)\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-28T17:23:18.633705Z","iopub.execute_input":"2022-06-28T17:23:18.633974Z","iopub.status.idle":"2022-06-28T17:23:18.652518Z","shell.execute_reply.started":"2022-06-28T17:23:18.633946Z","shell.execute_reply":"2022-06-28T17:23:18.651572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline with **tf.data**\n<div align=center> <img src=\"https://storage.googleapis.com/jalammar-ml/tf.data/images/tf.data.png\" width=700></div>\n\nTo build data pipeline using `tfrecrod/tfrec`, we need to use `tf.data` API.\n\n* We can build complex input pipelines from simple, reusable pieces using`tf.data` API . For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random transformation/augmentation to each image, and merge randomly selected images into a batch for training.\n* Moreover `tf.data` API provides a `tf.data.Dataset` feature that represents a sequence of components where each component comprises one or more pieces. For instances, in an image pipeline, an component might be a single training example, with a pair of tensor pieces representing the image and its label.\n\nCheckout this [doc](https://www.tensorflow.org/guide/data) if you want to learn more about `tf.data`.\n\n## Pipeline\n* Read **TFRecord** files.\n* `cache` data to speed up the training.\n* `repeat` the data stream (for training only & test-time augmentation).\n* `shuffle` the data (for training only).\n* Unparse **tfrecord** data & convert it to Image data from ByteString.\n* Process Image & Mask.\n* Apply Augmentations.\n* Batch Data.","metadata":{}},{"cell_type":"code","source":"def get_dataset(\n    filenames,\n    shuffle=True,\n    repeat=True,\n    augment=True,\n    cache=True,\n    return_id=False,\n    batch_size=CFG.batch_size * REPLICAS,\n    target_size=CFG.img_size,\n    drop_remainder=False,\n    seed=CFG.seed,\n):\n    dataset = tf.data.TFRecordDataset(\n        filenames, num_parallel_reads=AUTO\n    )  # read tfrecord files\n    if cache:\n        dataset = dataset.cache()  # cache data for speedup\n    if repeat:\n        dataset = dataset.repeat()  # repeat the data (for training only)\n    if shuffle:\n        dataset = dataset.shuffle(\n            1024, seed=seed\n        )  # shuffle the data (for training only)\n        options = tf.data.Options()\n        options.experimental_deterministic = (\n            False  # order won't be maintained when we shuffle\n        )\n        dataset = dataset.with_options(options)\n    dataset = dataset.map(\n        lambda x: read_tfrecord(\n            x,\n            augment=augment,  # unparse tfrecord data with masks\n            return_id=return_id,\n            dim=target_size,\n        ),\n        num_parallel_calls=AUTO,\n    )\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)  # batch the data\n    dataset = dataset.prefetch(AUTO)  # prefatch data for speedup\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:23:27.504533Z","iopub.execute_input":"2022-06-28T17:23:27.504836Z","iopub.status.idle":"2022-06-28T17:23:27.513709Z","shell.execute_reply.started":"2022-06-28T17:23:27.504805Z","shell.execute_reply":"2022-06-28T17:23:27.513125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization \nTo ensure our pipeline is generating **image** and **mask** correctly, we'll check some samples from a batch.","metadata":{}},{"cell_type":"code","source":"clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\ndef display_batch(batch, row=2, col=5):\n    imgs, msks = batch\n    plt.figure(figsize=(2.5*col, 2.5*row))\n    for idx in range(row*col):\n        ax = plt.subplot(row, col, idx+1)\n        img = imgs[idx].numpy()*255.0\n        img = img.astype('uint8')\n        for i in range(3):\n            img[...,i] = clahe.apply(img[...,i])\n        ax.imshow(img, cmap='bone')\n        msk = msks[idx].numpy()\n        ax.imshow(msk,alpha=0.4)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.tight_layout();\n    plt.show();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-28T17:23:33.066564Z","iopub.execute_input":"2022-06-28T17:23:33.066893Z","iopub.status.idle":"2022-06-28T17:23:33.080054Z","shell.execute_reply.started":"2022-06-28T17:23:33.066862Z","shell.execute_reply":"2022-06-28T17:23:33.079387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = get_dataset(ALL_FILENAMES[:2], augment=False, cache=False, repeat=False).take(1)\nbatch = next(iter(ds.unbatch().batch(20)))\nimg, msk = batch\nprint(f'image_shape: {img.shape} mask_shape:{msk.shape}')\nprint(f'image_dtype: {img.dtype} mask_dtype: {msk.dtype}')\ndisplay_batch(batch)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:23:35.80885Z","iopub.execute_input":"2022-06-28T17:23:35.809144Z","iopub.status.idle":"2022-06-28T17:23:46.18065Z","shell.execute_reply.started":"2022-06-28T17:23:35.809114Z","shell.execute_reply":"2022-06-28T17:23:46.179812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Fn \nSome implemented loss_functions are,\n* Dice Loss\n$$ \nDice = \\frac{2\\cdot{TP}}{2\\cdot{TP} + FP + FN}\n$$\n* Tversky Loss (Modified IoU Loss)\n$$ \nIoU = \\frac{TP}{TP + FP + FN}\n$$\n$$\nTversky = \\frac{TP}{TP + \\alpha\\cdot{FP} + \\beta\\cdot{FN}}\n$$\n* Focal Tversky Loss (Focal Loss + Tversky Loss)\n$$ \nFocalTversky = (1 - Tversky)^\\gamma\n$$","metadata":{}},{"cell_type":"code","source":"from segmentation_models.base import functional as F\nimport tensorflow.keras.backend as K\n\nkwargs = {}\nkwargs[\"backend\"] = K  # set tensorflow.keras as backend\n\n\ndef dice_coef(y_true, y_pred):\n    \"\"\"Dice coefficient\"\"\"\n    dice = F.f_score(\n        y_true,\n        y_pred,\n        beta=1,\n        smooth=1e-5,\n        per_image=False,\n        threshold=0.5,\n        **kwargs,\n    )\n    return dice\n\n\ndef tversky(y_true, y_pred, axis=(0, 1, 2), alpha=0.3, beta=0.7, smooth=0.0001):\n    \"Tversky metric\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    tp = tf.math.reduce_sum(y_true * y_pred, axis=axis) # calculate True Positive\n    fn = tf.math.reduce_sum(y_true * (1 - y_pred), axis=axis) # calculate False Negative\n    fp = tf.math.reduce_sum((1 - y_true) * y_pred, axis=axis) # calculate False Positive\n    tv = (tp + smooth) / (tp + alpha * fn + beta * fp + smooth) # calculate tversky\n    tv = tf.math.reduce_mean(tv)\n    return tv\n\n\ndef tversky_loss(y_true, y_pred):\n    \"Tversky Loss\"\n    return 1 - tversky(y_true, y_pred)\n\n\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n    \"Focal Tversky Loss: Focal Loss + Tversky Loss\"\n    tv = tversky(y_true, y_pred)\n    return K.pow((1 - tv), gamma)\n\n\n# Register custom objects\ncustom_objs = {\n    \"dice_loss\": sm.losses.dice_loss,\n    \"dice_coef\": dice_coef,\n    \"bce_dice_loss\": sm.losses.bce_dice_loss,\n    \"bce_jaccard_loss\": sm.losses.bce_jaccard_loss,\n    \"tversky_loss\": tversky_loss,\n    \"focal_tversky_loss\": focal_tversky_loss,\n    \"jaccard_loss\": sm.losses.jaccard_loss,\n    \"precision\": sm.metrics.precision,\n    \"recall\": sm.metrics.recall,\n}\ntf.keras.utils.get_custom_objects().update(custom_objs)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-28T17:23:55.88061Z","iopub.execute_input":"2022-06-28T17:23:55.88106Z","iopub.status.idle":"2022-06-28T17:23:55.894276Z","shell.execute_reply.started":"2022-06-28T17:23:55.88103Z","shell.execute_reply":"2022-06-28T17:23:55.893467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LR Schedule \n* Learning Rate scheduler for transfer learning. \n* The learning rate starts from `initial_learning_rate`, then decreases to a`minimum_learning_rate` using different methods namely,\n    * **ReduceLROnPlateau**: Reduce lr when score isn't improving.\n    * **CosineDecay**: Follow Cosine graph to reduce lr.\n    * **ExponentialDecay**: Reduce lr exponentially.","metadata":{}},{"cell_type":"code","source":"def get_lr_callback():\n    if CFG.lr_schedule == \"ReduceLROnPlateau\":\n        lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.1,\n            patience=int(CFG.patience / 2),\n            min_lr=CFG.lr / 1e2,\n        )\n    elif CFG.lr_schedule == \"CosineDecay\":\n        lr_schedule = tf.keras.experimental.CosineDecay(\n            initial_learning_rate=CFG.lr, decay_steps=CFG.epochs + 2, alpha=CFG.lr / 1e2\n        )\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0)\n    elif CFG.lr_schedule == \"ExponentialDecay\":\n        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n            initial_learning_rate=CFG.lr,\n            decay_steps=CFG.epochs + 2,\n            decay_rate=0.05,\n            staircase=False,\n        )\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0)\n    return lr_schedule","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:24:01.4725Z","iopub.execute_input":"2022-06-28T17:24:01.472769Z","iopub.status.idle":"2022-06-28T17:24:01.480234Z","shell.execute_reply.started":"2022-06-28T17:24:01.472742Z","shell.execute_reply":"2022-06-28T17:24:01.479621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TransUnet \n> [TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation](https://arxiv.org/pdf/2102.04306.pdf)\n\nIn a nutshell, this work shows how to convert a **UNet** into a so-called **TransUNet** by using a visual transformer (ViT) network in the encoder. Details of the architecture are in Figure below. As opposed to other methods which use a pure **transformer-based encoder** to convert the input image into a latent vector. A series of convolutions (much like in the original UNet) is used to convert the input image into a set of lower-resolution feature maps which are then encode with a ViT.\nSo, main components fo **TransUNet** are,\n\n1. Encoder (Pure or Hybrid)\n    * CNN\n    * Transformer\n2. Decoder\n    * CNN\n3. Skip Connection in Hybrid\n    * Connection betwween CNN Encoder & CNN Decoder\n\n> **Codes below are adapted from [here](https://github.com/kenza-bouzid/TransUnet)**\n\n<img src=\"https://production-media.paperswithcode.com/social-images/hfPJrzzvUuaeIMvb.png\" width=800>","metadata":{}},{"cell_type":"markdown","source":"## Encoder\nKey components of Encoder are,\n* AddPositionEmbs\n* MultiHeadSelfAttention\n* TransformerBlock\n* ResNet_Embeddings","metadata":{}},{"cell_type":"markdown","source":"## Decoder\nKey components of Decoders are,\n* SegmentationHead\n* Conv2DReLu\n* DecoderBlock\n* DecoderCup","metadata":{}},{"cell_type":"markdown","source":"## Model\n* Merge Encoder & Decoder part of TransUNet Model.\n* You can also ditch so many lines of code above for **TransUNet** model and load model directly using simple two lines of code,\n```py\nfrom transunet import TransUNet\nmodel = TransUNet(image_size=224, pretrain=True)\n```","metadata":{}},{"cell_type":"markdown","source":"## Build Model\n* Build complete model.\n* Select Loss, LR_Scheduling, Metrics and so on.\n* Compile model for training.","metadata":{}},{"cell_type":"code","source":"from transunet import TransUNet","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:25:37.585132Z","iopub.execute_input":"2022-06-28T17:25:37.585422Z","iopub.status.idle":"2022-06-28T17:25:37.713556Z","shell.execute_reply.started":"2022-06-28T17:25:37.585393Z","shell.execute_reply":"2022-06-28T17:25:37.712728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(name=CFG.model_name, loss=CFG.loss, backbone=CFG.backbone):\n    model = TransUNet(image_size=CFG.img_size[0], freeze_enc_cnn=False, pretrain=True)\n\n    lr = CFG.lr\n    if CFG.optimizer == \"Adam\":\n        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n    elif CFG.optimizer == \"AdamW\":\n        opt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=lr)\n    elif CFG.optimizer == \"RectifiedAdam\":\n        opt = tfa.optimizers.RectifiedAdam(learning_rate=lr)\n    else:\n        raise ValueError(\"Wrong Optimzer Name\")\n\n    model.compile(\n        optimizer=opt,\n        loss=loss,\n        steps_per_execution=CFG.steps_per_execution, # to reduce idle time\n        metrics=[\n            dice_coef,\n            \"precision\",\n            \"recall\",\n        ],\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:25:51.563391Z","iopub.execute_input":"2022-06-28T17:25:51.563957Z","iopub.status.idle":"2022-06-28T17:25:51.571276Z","shell.execute_reply.started":"2022-06-28T17:25:51.563904Z","shell.execute_reply":"2022-06-28T17:25:51.570735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-28T17:26:33.860834Z","iopub.execute_input":"2022-06-28T17:26:33.861128Z","iopub.status.idle":"2022-06-28T17:26:47.769157Z","shell.execute_reply.started":"2022-06-28T17:26:33.861099Z","shell.execute_reply":"2022-06-28T17:26:47.767899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training \nOur model will be trained for the number of `FOLDS` and `EPOCHS` you chose in the configuration above. Each fold the model with hightest validation `Dice Score` will be saved and used to predict OOF and test. ","metadata":{}},{"cell_type":"code","source":"M = {}\n# Which Metrics to store\nmetrics = [\n    \"loss\",\n    \"dice_coef\",\n    \"precision\",\n    \"recall\",\n]\n# Intialize Metrics\nfor fm in metrics:\n    M[\"val_\" + fm] = []\n\nALL_FILENAMES = sorted(ALL_FILENAMES)\n\n# Split tfrecord using KFold\nkf = KFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed) # kfold between trrecord files\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(ALL_FILENAMES)):\n    # If fold is not in selected folds then avoid that fold\n    if fold not in CFG.selected_folds:\n        continue\n        \n    # Train and validation files\n    TRAIN_FILENAMES = [ALL_FILENAMES[i] for i in train_idx]\n    VALID_FILENAMES = [ALL_FILENAMES[i] for i in valid_idx]\n    \n    # Take Only 10 Files if run in Debug Mode\n    if CFG.debug:\n        TRAIN_FILENAMES = TRAIN_FILENAMES[:10]\n        VALID_FILENAMES = VALID_FILENAMES[:10]\n\n    # Shuffle train files\n    random.shuffle(TRAIN_FILENAMES)\n\n    # Count train and valid samples\n    NUM_TRAIN = count_data_items(TRAIN_FILENAMES)\n    NUM_VALID = count_data_items(VALID_FILENAMES)\n\n    # Compute batch size & steps_per_epoch\n    BATCH_SIZE = CFG.batch_size * REPLICAS\n    STEPS_PER_EPOCH = NUM_TRAIN // BATCH_SIZE\n\n    print(\"#\" * 65)\n    print(\"#### FOLD:\", fold)\n    print(\n        \"#### IMAGE_SIZE: (%i, %i) | BATCH_SIZE: %i | EPOCHS: %i\"\n        % (CFG.img_size[0], CFG.img_size[1], BATCH_SIZE, CFG.epochs)\n    )\n    print(\n        \"#### MODEL: %s | BACKBONE: %s | LOSS: %s\"\n        % (CFG.model_name, CFG.backbone, CFG.loss)\n    )\n    print(\"#### NUM_TRAIN: {:,} | NUM_VALID: {:,}\".format(NUM_TRAIN, NUM_VALID))\n    print(\"#\" * 65)\n\n   \n    # Build model in device\n    K.clear_session()\n    with strategy.scope():\n        model = get_model(name=CFG.model_name, backbone=CFG.backbone, loss=CFG.loss)\n\n    # Callbacks\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        \"/kaggle/working/fold-%i.h5\" % fold,\n        verbose=CFG.verbose,\n        monitor=\"val_dice_coef\",\n        mode=\"max\",\n        save_best_only=True,\n        save_weights_only=True,\n    )\n    callbacks = [checkpoint, get_lr_callback()]\n\n    # Create train & valid dataset\n    train_ds = get_dataset(\n        TRAIN_FILENAMES,\n        augment=CFG.augment,\n        batch_size=BATCH_SIZE,\n        cache=False,\n        drop_remainder=False,\n    )\n    valid_ds = get_dataset(\n        VALID_FILENAMES,\n        shuffle=False,\n        augment=False,\n        repeat=False,\n        batch_size=BATCH_SIZE,\n        cache=False,\n        drop_remainder=False,\n    )\n\n    # Train model\n    history = model.fit(\n        train_ds,\n        epochs=CFG.epochs if not CFG.debug else 2,\n        steps_per_epoch=STEPS_PER_EPOCH,\n        callbacks=callbacks,\n        validation_data=valid_ds,\n        #         validation_steps = NUM_VALID/BATCH_SIZE,\n        verbose=CFG.verbose,\n    )\n\n    # Convert dict history to df history\n    history = pd.DataFrame(history.history)\n\n    # Load best weights\n    model.load_weights(\"/kaggle/working/fold-%i.h5\" % fold)\n\n    # Compute & save best valid result\n    print(\"\\nValid Result:\")\n    m = model.evaluate(\n        get_dataset(\n            VALID_FILENAMES,\n            batch_size=BATCH_SIZE,\n            augment=False,\n            shuffle=False,\n            repeat=False,\n            cache=False,\n        ),\n        return_dict=True,\n#        steps=NUM_VALID/BATCH_SIZE,\n        verbose=1,\n    )\n    print()\n    \n    # Store valid results\n    for fm in metrics:\n        M[\"val_\" + fm].append(m[fm])\n        \n \n    # Plot Training History\n    if CFG.display_plot:\n        plt.figure(figsize=(15, 5))\n        plt.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"dice_coef\"],\n            \"-o\",\n            label=\"Train Dice\",\n            color=\"#ff7f0e\",\n        )\n        plt.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"val_dice_coef\"],\n            \"-o\",\n            label=\"Val Dice\",\n            color=\"#1f77b4\",\n        )\n        x = np.argmax(history[\"val_dice_coef\"])\n        y = np.max(history[\"val_dice_coef\"])\n        xdist = plt.xlim()[1] - plt.xlim()[0]\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x, y, s=200, color=\"#1f77b4\")\n        plt.text(x - 0.03 * xdist, y - 0.13 * ydist, \"max dice\\n%.2f\" % y, size=14)\n        plt.ylabel(\"dice_coef\", size=14)\n        plt.xlabel(\"Epoch\", size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"loss\"],\n            \"-o\",\n            label=\"Train Loss\",\n            color=\"#2ca02c\",\n        )\n        plt2.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"val_loss\"],\n            \"-o\",\n            label=\"Val Loss\",\n            color=\"#d62728\",\n        )\n        x = np.argmin(history[\"val_loss\"])\n        y = np.min(history[\"val_loss\"])\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x, y, s=200, color=\"#d62728\")\n        plt.text(x - 0.03 * xdist, y + 0.05 * ydist, \"min loss\", size=14)\n        plt.ylabel(\"Loss\", size=14)\n        plt.title(\"FOLD %i\" % (fold), size=18)\n        plt.legend(loc=3)\n        plt.savefig(f\"fig-{fold}.png\")\n        plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-28T17:31:46.810748Z","iopub.execute_input":"2022-06-28T17:31:46.811306Z","iopub.status.idle":"2022-06-28T18:16:56.591702Z","shell.execute_reply.started":"2022-06-28T17:31:46.811272Z","shell.execute_reply":"2022-06-28T18:16:56.590701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 15. Calculate OOF ðŸ‘€\nLet's check our average score across all folds. This will help us compare our model's performance.","metadata":{}},{"cell_type":"code","source":"# Save Metrics\nM['datetime'] = str(datetime.now())\nfor fm in metrics:\n    M['oof_'+fm] = np.mean(M['val_'+fm])\n    print('OOF '+ fm + ': '+ str(M['oof_'+fm]))\nwith open('metrics.json', 'w') as outfile:\n    json.dump(M, outfile)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-28T18:21:19.061437Z","iopub.execute_input":"2022-06-28T18:21:19.062486Z","iopub.status.idle":"2022-06-28T18:21:19.071979Z","shell.execute_reply.started":"2022-06-28T18:21:19.062433Z","shell.execute_reply":"2022-06-28T18:21:19.071119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load best weights\nmodel.load_weights(\"/kaggle/working/fold-0.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:39:34.106637Z","iopub.execute_input":"2022-06-28T18:39:34.10695Z","iopub.status.idle":"2022-06-28T18:39:39.262409Z","shell.execute_reply.started":"2022-06-28T18:39:34.106919Z","shell.execute_reply":"2022-06-28T18:39:39.260782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"TransUNet_TF_weights\")\n#model.save(\"TransUNet_TF.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:43:44.432813Z","iopub.execute_input":"2022-06-28T18:43:44.43365Z","iopub.status.idle":"2022-06-28T18:43:49.837202Z","shell.execute_reply.started":"2022-06-28T18:43:44.433606Z","shell.execute_reply":"2022-06-28T18:43:49.835834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infer(model_paths, test_loader, num_log=1, thr=CFG.thr):\n    msks = []; imgs = [];\n    pred_strings = []; pred_ids = []; pred_classes = [];\n    for idx, (img, ids, heights, widths) in enumerate(tqdm(test_loader, total=len(test_loader), desc='Infer ')):\n        img = img.to(CFG.device, dtype=torch.float) # .squeeze(0)\n        size = img.size()\n        msk = []\n        msk = torch.zeros((size[0], 3, size[2], size[3]), device=CFG.device, dtype=torch.float32)\n        for path in model_paths:\n            model = load_model(path)\n            out   = model(img) # .squeeze(0) # removing batch axis\n            out   = nn.Sigmoid()(out) # removing channel axis\n            msk+=out/len(model_paths)\n        msk = (msk.permute((0,2,3,1))>thr).to(torch.uint8).cpu().detach().numpy() # shape: (n, h, w, c)\n        result = masks2rles(msk, ids, heights, widths)\n        pred_strings.extend(result[0])\n        pred_ids.extend(result[1])\n        pred_classes.extend(result[2])\n        if idx<num_log:\n            img = img.permute((0,2,3,1)).cpu().detach().numpy()\n            imgs.append(img[:10])\n            msks.append(msk[:10])\n        del img, msk, out, model, result\n        gc.collect()\n        torch.cuda.empty_cache()\n    return pred_strings, pred_ids, pred_classes, imgs, msks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = BuildDataset(test_df, transforms=data_transforms['valid'])\ntest_loader  = DataLoader(test_dataset, batch_size=CFG.valid_bs, \n                          num_workers=4, shuffle=False, pin_memory=False)\nmodel_paths  = glob(f'{CKPT_DIR}/best_epoch*.bin')\npred_strings, pred_ids, pred_classes, imgs, msks = infer(model_paths, test_loader)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 16. Reference ðŸ’¡\n* [Triple Stratified KFold with TFRecords](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords) by [Chris Deotte](https://www.kaggle.com/cdeotte)\n* [TransUNet](https://github.com/Beckschen/TransUNet)(Official)\n* [TransUnet](https://github.com/kenza-bouzid/TransUnet)(Keras)","metadata":{}}]}